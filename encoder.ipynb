{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model, save_model\n",
    "from tensorflow.keras.layers import Dense, Input, GlobalAveragePooling1D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from transformer.encoder import Encoder\n",
    "from transformer.utils import WordTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {'num_heads': 12, \n",
    "          'vocab_size': 30522,\n",
    "          'hidden_size': 128,\n",
    "          'max_position_embeds': 512,\n",
    "          'intermediate_size': 512,\n",
    "          'dropout_p': 0.1,\n",
    "          'input_size': (100,),\n",
    "          'num_hidden_layers': 1}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get some toy data\n",
    "\n",
    "The yelp sentence sentiment data set from Kaggle will do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Wow... Loved this place.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Crust is not good.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Not tasty and the texture was just nasty.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Stopped by during the late May bank holiday of...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The selection on the menu was great and so wer...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence  sentiment\n",
       "0                           Wow... Loved this place.          1\n",
       "1                                 Crust is not good.          0\n",
       "2          Not tasty and the texture was just nasty.          0\n",
       "3  Stopped by during the late May bank holiday of...          1\n",
       "4  The selection on the menu was great and so wer...          1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rows = []\n",
    "with open('yelp_labelled.txt', 'r') as FILE:\n",
    "    while True:\n",
    "        row = FILE.readline()\n",
    "        if not row:\n",
    "            break\n",
    "        row = row.strip().split('\\t')\n",
    "        sentence = row[0]\n",
    "        sentiment = int(row[1])\n",
    "        rows.append({'sentence': sentence, 'sentiment': sentiment})\n",
    " \n",
    "df = pd.DataFrame(rows, columns=['sentence', 'sentiment'])\n",
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize the sentences and split into x and y \n",
    "\n",
    "y will be the last token of each sequence, so we can try to predict it. But mostly we just want to see if our transformer encoder trains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences: 1000\n",
      "Vocabulary size: 2971\n"
     ]
    }
   ],
   "source": [
    "tokenizer = WordTokenizer(make_mask=True)\n",
    "config['vocab_size'] = tokenizer.fit(df['sentence'].values)\n",
    "tokens, masks = tokenizer.transform(df['sentence'].values,\n",
    "                             max_len=config['input_size'][0])\n",
    "\n",
    "X = np.array([[0] + s[:-1] for s in tokens.tolist()])\n",
    "y = np.array([s[-1] for s in tokens])\n",
    "masks = np.array([[0] + m[:-1] for m in masks.tolist()])\n",
    "\n",
    "print(f'Number of sentences: {df.shape[0]}')\n",
    "print(f'Vocabulary size: {config[\"vocab_size\"]}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try our transformer!\n",
    "\n",
    "We'll just train our encoder by the task of predicting the last word of each sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 100)]        0           []                               \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)           [(None, 100)]        0           []                               \n",
      "                                                                                                  \n",
      " Encoder (Encoder)              (None, 100, 128)     640232      ['input_1[0][0]',                \n",
      "                                                                  'input_2[0][0]']                \n",
      "                                                                                                  \n",
      " global_average_pooling1d (Glob  (None, 128)         0           ['Encoder[0][0]']                \n",
      " alAveragePooling1D)                                                                              \n",
      "                                                                                                  \n",
      " dense_39 (Dense)               (None, 2971)         383259      ['global_average_pooling1d[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 1,023,491\n",
      "Trainable params: 1,023,491\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "token_input = Input(shape=config['input_size'])\n",
    "mask_input = Input(shape=config['input_size'])\n",
    "x = Encoder(config)([token_input, mask_input])\n",
    "x = GlobalAveragePooling1D()(x)\n",
    "output = Dense(config['vocab_size'], activation='softmax')(x)\n",
    "\n",
    "clf = Model(inputs=[token_input, mask_input], outputs=output)\n",
    "\n",
    "clf.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "32/32 [==============================] - 14s 145ms/step - loss: 7.7310\n",
      "Epoch 2/200\n",
      "32/32 [==============================] - 5s 149ms/step - loss: 6.4961\n",
      "Epoch 3/200\n",
      "32/32 [==============================] - 5s 143ms/step - loss: 6.2537\n",
      "Epoch 4/200\n",
      "32/32 [==============================] - 4s 139ms/step - loss: 6.1788\n",
      "Epoch 5/200\n",
      "32/32 [==============================] - 4s 139ms/step - loss: 6.0760\n",
      "Epoch 6/200\n",
      "32/32 [==============================] - 5s 141ms/step - loss: 5.9436\n",
      "Epoch 7/200\n",
      "32/32 [==============================] - 5s 141ms/step - loss: 5.7278\n",
      "Epoch 8/200\n",
      "32/32 [==============================] - 5s 151ms/step - loss: 5.3913\n",
      "Epoch 9/200\n",
      "32/32 [==============================] - 5s 155ms/step - loss: 4.9447\n",
      "Epoch 10/200\n",
      "32/32 [==============================] - 5s 151ms/step - loss: 4.2744\n",
      "Epoch 11/200\n",
      "32/32 [==============================] - 5s 143ms/step - loss: 3.5494\n",
      "Epoch 12/200\n",
      "32/32 [==============================] - 5s 143ms/step - loss: 2.8348\n",
      "Epoch 13/200\n",
      "32/32 [==============================] - 5s 144ms/step - loss: 2.1739\n",
      "Epoch 14/200\n",
      "32/32 [==============================] - 5s 142ms/step - loss: 1.5497\n",
      "Epoch 15/200\n",
      "32/32 [==============================] - 5s 144ms/step - loss: 1.0907\n",
      "Epoch 16/200\n",
      "32/32 [==============================] - 5s 142ms/step - loss: 0.7802\n",
      "Epoch 17/200\n",
      "32/32 [==============================] - 5s 146ms/step - loss: 0.6008\n",
      "Epoch 18/200\n",
      "32/32 [==============================] - 5s 148ms/step - loss: 0.4540\n",
      "Epoch 19/200\n",
      "32/32 [==============================] - 5s 146ms/step - loss: 0.3347\n",
      "Epoch 20/200\n",
      "32/32 [==============================] - 5s 142ms/step - loss: 0.2898\n",
      "Epoch 21/200\n",
      "32/32 [==============================] - 5s 145ms/step - loss: 0.2569\n",
      "Epoch 22/200\n",
      "32/32 [==============================] - 5s 145ms/step - loss: 0.2107\n",
      "Epoch 23/200\n",
      "32/32 [==============================] - 5s 145ms/step - loss: 0.1773\n",
      "Epoch 24/200\n",
      "32/32 [==============================] - 5s 141ms/step - loss: 0.1348\n",
      "Epoch 25/200\n",
      "32/32 [==============================] - 5s 155ms/step - loss: 0.1314\n",
      "Epoch 26/200\n",
      "32/32 [==============================] - 5s 148ms/step - loss: 0.1264\n",
      "Epoch 27/200\n",
      "32/32 [==============================] - 5s 151ms/step - loss: 0.1189\n",
      "Epoch 28/200\n",
      "32/32 [==============================] - 5s 142ms/step - loss: 0.1243\n",
      "Epoch 29/200\n",
      "32/32 [==============================] - 4s 140ms/step - loss: 0.0981\n",
      "Epoch 30/200\n",
      "32/32 [==============================] - 5s 146ms/step - loss: 0.1000\n",
      "Epoch 31/200\n",
      "32/32 [==============================] - 5s 152ms/step - loss: 0.1129\n",
      "Epoch 32/200\n",
      "32/32 [==============================] - 5s 152ms/step - loss: 0.1035\n",
      "Epoch 33/200\n",
      "32/32 [==============================] - 5s 158ms/step - loss: 0.1061\n",
      "Epoch 34/200\n",
      "32/32 [==============================] - 5s 142ms/step - loss: 0.0956\n",
      "Epoch 35/200\n",
      "32/32 [==============================] - 4s 139ms/step - loss: 0.0882\n",
      "Epoch 36/200\n",
      "32/32 [==============================] - 4s 133ms/step - loss: 0.0847\n",
      "Epoch 37/200\n",
      "32/32 [==============================] - 4s 135ms/step - loss: 0.0843\n",
      "Epoch 38/200\n",
      "32/32 [==============================] - 4s 136ms/step - loss: 0.0863\n",
      "Epoch 39/200\n",
      "32/32 [==============================] - 4s 139ms/step - loss: 0.0835\n",
      "Epoch 40/200\n",
      "32/32 [==============================] - 4s 138ms/step - loss: 0.0819\n",
      "Epoch 41/200\n",
      "32/32 [==============================] - 4s 137ms/step - loss: 0.0639\n",
      "Epoch 42/200\n",
      "32/32 [==============================] - 4s 134ms/step - loss: 0.0725\n",
      "Epoch 43/200\n",
      "32/32 [==============================] - 5s 140ms/step - loss: 0.0510\n",
      "Epoch 44/200\n",
      "32/32 [==============================] - 4s 137ms/step - loss: 0.0630\n",
      "Epoch 45/200\n",
      "32/32 [==============================] - 4s 138ms/step - loss: 0.0676\n",
      "Epoch 46/200\n",
      "32/32 [==============================] - 4s 139ms/step - loss: 0.0738\n",
      "Epoch 47/200\n",
      "32/32 [==============================] - 4s 135ms/step - loss: 0.0589\n",
      "Epoch 48/200\n",
      "32/32 [==============================] - 4s 139ms/step - loss: 0.0477\n",
      "Epoch 49/200\n",
      "32/32 [==============================] - 4s 133ms/step - loss: 0.0744\n",
      "Epoch 50/200\n",
      "32/32 [==============================] - 4s 134ms/step - loss: 0.0644\n",
      "Epoch 51/200\n",
      "32/32 [==============================] - 4s 137ms/step - loss: 0.0592\n",
      "Epoch 52/200\n",
      "32/32 [==============================] - 4s 135ms/step - loss: 0.0531\n",
      "Epoch 53/200\n",
      "32/32 [==============================] - 4s 134ms/step - loss: 0.0534\n",
      "Epoch 54/200\n",
      "32/32 [==============================] - 4s 135ms/step - loss: 0.0608\n",
      "Epoch 55/200\n",
      "32/32 [==============================] - 5s 151ms/step - loss: 0.0530\n",
      "Epoch 56/200\n",
      "32/32 [==============================] - 5s 149ms/step - loss: 0.0664\n",
      "Epoch 57/200\n",
      "32/32 [==============================] - 5s 148ms/step - loss: 0.0382\n",
      "Epoch 58/200\n",
      "32/32 [==============================] - 5s 146ms/step - loss: 0.0428\n",
      "Epoch 59/200\n",
      "32/32 [==============================] - 5s 145ms/step - loss: 0.0464\n",
      "Epoch 60/200\n",
      "32/32 [==============================] - 5s 146ms/step - loss: 0.0356\n",
      "Epoch 61/200\n",
      "32/32 [==============================] - 5s 147ms/step - loss: 0.0627\n",
      "Epoch 62/200\n",
      "32/32 [==============================] - 5s 147ms/step - loss: 0.0596\n",
      "Epoch 63/200\n",
      "32/32 [==============================] - 5s 147ms/step - loss: 0.0450\n",
      "Epoch 64/200\n",
      "32/32 [==============================] - 5s 144ms/step - loss: 0.0371\n",
      "Epoch 65/200\n",
      "32/32 [==============================] - 5s 146ms/step - loss: 0.0461\n",
      "Epoch 66/200\n",
      "32/32 [==============================] - 5s 147ms/step - loss: 0.0382\n",
      "Epoch 67/200\n",
      "32/32 [==============================] - 5s 148ms/step - loss: 0.0453\n",
      "Epoch 68/200\n",
      "32/32 [==============================] - 5s 146ms/step - loss: 0.0571\n",
      "Epoch 69/200\n",
      "32/32 [==============================] - 5s 146ms/step - loss: 0.0550\n",
      "Epoch 70/200\n",
      "32/32 [==============================] - 5s 144ms/step - loss: 0.0531\n",
      "Epoch 71/200\n",
      "32/32 [==============================] - 5s 148ms/step - loss: 0.0410\n",
      "Epoch 72/200\n",
      "32/32 [==============================] - 5s 145ms/step - loss: 0.0466\n",
      "Epoch 73/200\n",
      "32/32 [==============================] - 5s 145ms/step - loss: 0.0408\n",
      "Epoch 74/200\n",
      "32/32 [==============================] - 5s 145ms/step - loss: 0.0361\n",
      "Epoch 75/200\n",
      "32/32 [==============================] - 5s 147ms/step - loss: 0.0394\n",
      "Epoch 76/200\n",
      "32/32 [==============================] - 5s 154ms/step - loss: 0.0357\n",
      "Epoch 77/200\n",
      "32/32 [==============================] - 5s 151ms/step - loss: 0.0514\n",
      "Epoch 78/200\n",
      "32/32 [==============================] - 4s 140ms/step - loss: 0.0342\n",
      "Epoch 79/200\n",
      "32/32 [==============================] - 4s 138ms/step - loss: 0.0371\n",
      "Epoch 80/200\n",
      "32/32 [==============================] - 5s 144ms/step - loss: 0.0400\n",
      "Epoch 81/200\n",
      "32/32 [==============================] - 4s 140ms/step - loss: 0.0478\n",
      "Epoch 82/200\n",
      "32/32 [==============================] - 4s 139ms/step - loss: 0.0491\n",
      "Epoch 83/200\n",
      "32/32 [==============================] - 4s 140ms/step - loss: 0.0415\n",
      "Epoch 84/200\n",
      "32/32 [==============================] - 4s 139ms/step - loss: 0.0375\n",
      "Epoch 85/200\n",
      "32/32 [==============================] - 5s 151ms/step - loss: 0.0410\n",
      "Epoch 86/200\n",
      "32/32 [==============================] - 5s 143ms/step - loss: 0.0617\n",
      "Epoch 87/200\n",
      "32/32 [==============================] - 5s 151ms/step - loss: 0.0544\n",
      "Epoch 88/200\n",
      "32/32 [==============================] - 5s 145ms/step - loss: 0.0651\n",
      "Epoch 89/200\n",
      "32/32 [==============================] - 5s 146ms/step - loss: 0.0553\n",
      "Epoch 90/200\n",
      "32/32 [==============================] - 5s 146ms/step - loss: 0.0582\n",
      "Epoch 91/200\n",
      "32/32 [==============================] - 5s 149ms/step - loss: 0.0532\n",
      "Epoch 92/200\n",
      "32/32 [==============================] - 5s 147ms/step - loss: 0.0751\n",
      "Epoch 93/200\n",
      "32/32 [==============================] - 5s 149ms/step - loss: 0.0519\n",
      "Epoch 94/200\n",
      "32/32 [==============================] - 5s 148ms/step - loss: 0.0555\n",
      "Epoch 95/200\n",
      "32/32 [==============================] - 5s 146ms/step - loss: 0.0406\n",
      "Epoch 96/200\n",
      "32/32 [==============================] - 5s 147ms/step - loss: 0.0447\n",
      "Epoch 97/200\n",
      "32/32 [==============================] - 5s 149ms/step - loss: 0.0451\n",
      "Epoch 98/200\n",
      "32/32 [==============================] - 5s 148ms/step - loss: 0.0445\n",
      "Epoch 99/200\n",
      "32/32 [==============================] - 5s 146ms/step - loss: 0.0743\n",
      "Epoch 100/200\n",
      "32/32 [==============================] - 5s 148ms/step - loss: 0.0710\n",
      "Epoch 101/200\n",
      "32/32 [==============================] - 5s 147ms/step - loss: 0.0615\n",
      "Epoch 102/200\n",
      "32/32 [==============================] - 5s 148ms/step - loss: 0.0453\n",
      "Epoch 103/200\n",
      "32/32 [==============================] - 5s 146ms/step - loss: 0.0322\n",
      "Epoch 104/200\n",
      "32/32 [==============================] - 5s 145ms/step - loss: 0.0350\n",
      "Epoch 105/200\n",
      "32/32 [==============================] - 5s 146ms/step - loss: 0.0494\n",
      "Epoch 106/200\n",
      "32/32 [==============================] - 5s 146ms/step - loss: 0.0581\n",
      "Epoch 107/200\n",
      "32/32 [==============================] - 5s 146ms/step - loss: 0.0417\n",
      "Epoch 108/200\n",
      "32/32 [==============================] - 5s 146ms/step - loss: 0.0392\n",
      "Epoch 109/200\n",
      "32/32 [==============================] - 5s 148ms/step - loss: 0.0458\n",
      "Epoch 110/200\n",
      "32/32 [==============================] - 5s 145ms/step - loss: 0.0340\n",
      "Epoch 111/200\n",
      "32/32 [==============================] - 5s 146ms/step - loss: 0.0524\n",
      "Epoch 112/200\n",
      "32/32 [==============================] - 5s 147ms/step - loss: 0.0463\n",
      "Epoch 113/200\n",
      "32/32 [==============================] - 5s 148ms/step - loss: 0.0539\n",
      "Epoch 114/200\n",
      "32/32 [==============================] - 5s 145ms/step - loss: 0.0407\n",
      "Epoch 115/200\n",
      "32/32 [==============================] - 5s 146ms/step - loss: 0.0519\n",
      "Epoch 116/200\n",
      "32/32 [==============================] - 5s 145ms/step - loss: 0.0385\n",
      "Epoch 117/200\n",
      "32/32 [==============================] - 5s 146ms/step - loss: 0.0409\n",
      "Epoch 118/200\n",
      "32/32 [==============================] - 5s 146ms/step - loss: 0.0401\n",
      "Epoch 119/200\n",
      "32/32 [==============================] - 5s 146ms/step - loss: 0.0376\n",
      "Epoch 120/200\n",
      "32/32 [==============================] - 5s 147ms/step - loss: 0.0304\n",
      "Epoch 121/200\n",
      "32/32 [==============================] - 5s 146ms/step - loss: 0.0342\n",
      "Epoch 122/200\n",
      "32/32 [==============================] - 5s 146ms/step - loss: 0.0299\n",
      "Epoch 123/200\n",
      "32/32 [==============================] - 5s 146ms/step - loss: 0.0274\n",
      "Epoch 124/200\n",
      "32/32 [==============================] - 5s 149ms/step - loss: 0.0361\n",
      "Epoch 125/200\n",
      "32/32 [==============================] - 5s 145ms/step - loss: 0.0489\n",
      "Epoch 126/200\n",
      "32/32 [==============================] - 5s 150ms/step - loss: 0.0389\n",
      "Epoch 127/200\n",
      "32/32 [==============================] - 5s 148ms/step - loss: 0.0485\n",
      "Epoch 128/200\n",
      "32/32 [==============================] - 5s 148ms/step - loss: 0.0477\n",
      "Epoch 129/200\n",
      "32/32 [==============================] - 5s 148ms/step - loss: 0.0408\n",
      "Epoch 130/200\n",
      "32/32 [==============================] - 5s 149ms/step - loss: 0.0292\n",
      "Epoch 131/200\n",
      "32/32 [==============================] - 5s 146ms/step - loss: 0.0322\n",
      "Epoch 132/200\n",
      "32/32 [==============================] - 5s 146ms/step - loss: 0.0360\n",
      "Epoch 133/200\n",
      "32/32 [==============================] - 5s 148ms/step - loss: 0.0389\n",
      "Epoch 134/200\n",
      "32/32 [==============================] - 5s 145ms/step - loss: 0.0316\n",
      "Epoch 135/200\n",
      "32/32 [==============================] - 5s 146ms/step - loss: 0.0388\n",
      "Epoch 136/200\n",
      "32/32 [==============================] - 5s 146ms/step - loss: 0.0432\n",
      "Epoch 137/200\n",
      "32/32 [==============================] - 5s 148ms/step - loss: 0.0377\n",
      "Epoch 138/200\n",
      "32/32 [==============================] - 5s 151ms/step - loss: 0.0297\n",
      "Epoch 139/200\n",
      "32/32 [==============================] - 5s 150ms/step - loss: 0.0587\n",
      "Epoch 140/200\n",
      "32/32 [==============================] - 5s 148ms/step - loss: 0.0570\n",
      "Epoch 141/200\n",
      "32/32 [==============================] - 5s 146ms/step - loss: 0.0689\n",
      "Epoch 142/200\n",
      "32/32 [==============================] - 5s 148ms/step - loss: 0.0471\n",
      "Epoch 143/200\n",
      "32/32 [==============================] - 5s 151ms/step - loss: 0.0451\n",
      "Epoch 144/200\n",
      "32/32 [==============================] - 5s 147ms/step - loss: 0.0491\n",
      "Epoch 145/200\n",
      "32/32 [==============================] - 5s 148ms/step - loss: 0.0397\n",
      "Epoch 146/200\n",
      "32/32 [==============================] - 5s 146ms/step - loss: 0.0452\n",
      "Epoch 147/200\n",
      "32/32 [==============================] - 5s 149ms/step - loss: 0.0384\n",
      "Epoch 148/200\n",
      "32/32 [==============================] - 5s 147ms/step - loss: 0.0402\n",
      "Epoch 149/200\n",
      "32/32 [==============================] - 5s 146ms/step - loss: 0.0215\n",
      "Epoch 150/200\n",
      "32/32 [==============================] - 5s 148ms/step - loss: 0.0359\n",
      "Epoch 151/200\n",
      "32/32 [==============================] - 5s 147ms/step - loss: 0.0281\n",
      "Epoch 152/200\n",
      "32/32 [==============================] - 5s 145ms/step - loss: 0.0274\n",
      "Epoch 153/200\n",
      "32/32 [==============================] - 5s 148ms/step - loss: 0.0253\n",
      "Epoch 154/200\n",
      "32/32 [==============================] - 5s 147ms/step - loss: 0.0414\n",
      "Epoch 155/200\n",
      "32/32 [==============================] - 5s 145ms/step - loss: 0.0247\n",
      "Epoch 156/200\n",
      "32/32 [==============================] - 5s 152ms/step - loss: 0.0448\n",
      "Epoch 157/200\n",
      "32/32 [==============================] - 5s 149ms/step - loss: 0.0370\n",
      "Epoch 158/200\n",
      "32/32 [==============================] - 5s 146ms/step - loss: 0.0389\n",
      "Epoch 159/200\n",
      "32/32 [==============================] - 5s 146ms/step - loss: 0.0377\n",
      "Epoch 160/200\n",
      "32/32 [==============================] - 5s 152ms/step - loss: 0.0337\n",
      "Epoch 161/200\n",
      "32/32 [==============================] - 5s 146ms/step - loss: 0.0330\n",
      "Epoch 162/200\n",
      "32/32 [==============================] - 5s 147ms/step - loss: 0.0383\n",
      "Epoch 163/200\n",
      "32/32 [==============================] - 5s 149ms/step - loss: 0.0368\n",
      "Epoch 164/200\n",
      "32/32 [==============================] - 5s 146ms/step - loss: 0.0437\n",
      "Epoch 165/200\n",
      "32/32 [==============================] - 5s 146ms/step - loss: 0.0437\n",
      "Epoch 166/200\n",
      "32/32 [==============================] - 5s 150ms/step - loss: 0.0759\n",
      "Epoch 167/200\n",
      "32/32 [==============================] - 5s 147ms/step - loss: 0.0546\n",
      "Epoch 168/200\n",
      "32/32 [==============================] - 5s 148ms/step - loss: 0.0607\n",
      "Epoch 169/200\n",
      "32/32 [==============================] - 5s 148ms/step - loss: 0.0403\n",
      "Epoch 170/200\n",
      "32/32 [==============================] - 5s 150ms/step - loss: 0.0401\n",
      "Epoch 171/200\n",
      "32/32 [==============================] - 5s 146ms/step - loss: 0.0573\n",
      "Epoch 172/200\n",
      "32/32 [==============================] - 5s 147ms/step - loss: 0.0771\n",
      "Epoch 173/200\n",
      "32/32 [==============================] - 5s 149ms/step - loss: 0.0535\n",
      "Epoch 174/200\n",
      "32/32 [==============================] - 5s 146ms/step - loss: 0.0476\n",
      "Epoch 175/200\n",
      "32/32 [==============================] - 5s 147ms/step - loss: 0.0550\n",
      "Epoch 176/200\n",
      "32/32 [==============================] - 5s 149ms/step - loss: 0.0461\n",
      "Epoch 177/200\n",
      "32/32 [==============================] - 5s 146ms/step - loss: 0.0327\n",
      "Epoch 178/200\n",
      "32/32 [==============================] - 5s 147ms/step - loss: 0.0538\n",
      "Epoch 179/200\n",
      "32/32 [==============================] - 5s 148ms/step - loss: 0.0345\n",
      "Epoch 180/200\n",
      "32/32 [==============================] - 5s 148ms/step - loss: 0.0495\n",
      "Epoch 181/200\n",
      "32/32 [==============================] - 5s 147ms/step - loss: 0.0667\n",
      "Epoch 182/200\n",
      "32/32 [==============================] - 5s 151ms/step - loss: 0.0347\n",
      "Epoch 183/200\n",
      "32/32 [==============================] - 5s 147ms/step - loss: 0.0319\n",
      "Epoch 184/200\n",
      "32/32 [==============================] - 5s 146ms/step - loss: 0.0332\n",
      "Epoch 185/200\n",
      "32/32 [==============================] - 5s 148ms/step - loss: 0.0542\n",
      "Epoch 186/200\n",
      "32/32 [==============================] - 5s 146ms/step - loss: 0.0249\n",
      "Epoch 187/200\n",
      "32/32 [==============================] - 5s 147ms/step - loss: 0.0277\n",
      "Epoch 188/200\n",
      "32/32 [==============================] - 5s 148ms/step - loss: 0.0257\n",
      "Epoch 189/200\n",
      "32/32 [==============================] - 5s 147ms/step - loss: 0.0286\n",
      "Epoch 190/200\n",
      "32/32 [==============================] - 5s 147ms/step - loss: 0.0277\n",
      "Epoch 191/200\n",
      "32/32 [==============================] - 5s 149ms/step - loss: 0.0280\n",
      "Epoch 192/200\n",
      "32/32 [==============================] - 5s 146ms/step - loss: 0.0421\n",
      "Epoch 193/200\n",
      "32/32 [==============================] - 5s 146ms/step - loss: 0.0272\n",
      "Epoch 194/200\n",
      "32/32 [==============================] - 5s 149ms/step - loss: 0.0320\n",
      "Epoch 195/200\n",
      "32/32 [==============================] - 5s 147ms/step - loss: 0.0377\n",
      "Epoch 196/200\n",
      "32/32 [==============================] - 5s 146ms/step - loss: 0.0317\n",
      "Epoch 197/200\n",
      "32/32 [==============================] - 5s 148ms/step - loss: 0.0296\n",
      "Epoch 198/200\n",
      "32/32 [==============================] - 5s 146ms/step - loss: 0.0243\n",
      "Epoch 199/200\n",
      "32/32 [==============================] - 5s 146ms/step - loss: 0.0380\n",
      "Epoch 200/200\n",
      "32/32 [==============================] - 5s 149ms/step - loss: 0.0317\n"
     ]
    }
   ],
   "source": [
    "clf.compile(optimizer=Adam(learning_rate=0.0005), \n",
    "            loss='sparse_categorical_crossentropy')\n",
    "hist = clf.fit([X, masks],\n",
    "               y,\n",
    "               epochs=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 300x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(3,3))\n",
    "plt.plot(hist.history['loss'])\n",
    "plt.title('Training')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test our toy model\n",
    "\n",
    "Make up a few sentences that may belong to the distribution (Yelp restaraunt review sentences).\n",
    "\n",
    "Then, dropping off that last word of each, try to complete them. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 1s/step\n",
      "\n",
      "Original sentence: It really tasted awful.\n",
      "Completed sentence: It really tasted here!!!\"\n",
      "\n",
      "Original sentence: It was very good.\n",
      "Completed sentence: It was very good.\n",
      "\n",
      "Original sentence: It was awful.\n",
      "Completed sentence: It was packed!!\n",
      "\n",
      "Original sentence: This is a very bad place.\n",
      "Completed sentence: This is a very bad overpriced.\n",
      "\n",
      "Original sentence: The spaghetti was perfect\n",
      "Completed sentence: The spaghetti was huge!\n",
      "\n",
      "Original sentence: The eggs were gross!\n",
      "Completed sentence: The eggs were door.\n",
      "\n",
      "Original sentence: My steak was bad.\n",
      "Completed sentence: My steak was too!\n"
     ]
    }
   ],
   "source": [
    "s = [\"It really tasted awful.\",\n",
    "     \"It was very good.\",\n",
    "     \"It was awful.\",\n",
    "     \"This is a very bad place.\",\n",
    "     \"The spaghetti was perfect\",\n",
    "     \"The eggs were gross!\",\n",
    "     \"My steak was bad.\"]\n",
    "\n",
    "# Tokenize our sentence, separate last word, and pad\n",
    "tokens_test, masks_tests = tokenizer.transform(s, max_len=config['input_size'][0])\n",
    "\n",
    "X_test = np.array([[0] + s[:-1] for s in tokens_test.tolist()])\n",
    "y_test = np.array([s[-1] for s in tokens])\n",
    "masks_test = np.array([[0] + m[:-1] for m in masks_tests.tolist()])\n",
    "\n",
    "# Predict last word\n",
    "y_hat = clf.predict([X_test, masks_test])\n",
    "y_hat = np.argmax(y_hat, axis=1)\n",
    "\n",
    "for i in range(len(s)):\n",
    "     # construct predicted complete sentence\n",
    "     pred_s = X_test[i].tolist()\n",
    "     pred_s.append(y_hat[i])\n",
    "\n",
    "     #unpad\n",
    "     pred_s = [token for token in pred_s if token!= 0]\n",
    "     pred_s = ' '.join([tokenizer.vocab_reverse[i] for i in pred_s])\n",
    "     print(f'\\nOriginal sentence: {s[i]}\\nCompleted sentence: {pred_s}')\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save encoder model\n",
    "\n",
    "Omitting the classifier head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 100)]        0           []                               \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)           [(None, 100)]        0           []                               \n",
      "                                                                                                  \n",
      " Encoder (Encoder)              (None, 100, 128)     640232      ['input_1[0][0]',                \n",
      "                                                                  'input_2[0][0]']                \n",
      "                                                                                                  \n",
      " global_average_pooling1d (Glob  (None, 128)         0           ['Encoder[0][0]']                \n",
      " alAveragePooling1D)                                                                              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 640,232\n",
      "Trainable params: 640,232\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input1, input2 = clf.input\n",
    "encoder_model = tf.keras.models.Model(inputs=[input1, input2],\n",
    "                                      outputs=clf.layers[-2].output)\n",
    "encoder_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "save_model(encoder_model, 'encoder_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trans-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
